{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-G7hoJQmNQ_",
        "outputId": "e8f88fcb-8edb-486b-e0c0-8c382de25fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb 11 19:16:26 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8              13W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Word Tokenization**\n",
        "\n",
        "The NLTK word_tokenize method tokenizes the text into individual words, including contractions like \"I'm\" and emojis like \"🌍✈️\" treated as separate tokens."
      ],
      "metadata": {
        "id": "4UjbEcbOm_K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "text = \"As an avid traveler 🌍✈️, I'm constantly seeking new adventures and experiences that broaden my horizons. Whether it's trekking through rugged mountains ⛰️, exploring bustling city streets 🏙️, or relaxing on pristine beaches 🏖️, each journey leaves me with unforgettable memories and valuable lessons. However, amidst the excitement, it's essential to prioritize sustainability and responsible tourism practices. 🌱🌿 Let's tread lightly on this beautiful planet 🌏, leaving only footprints and taking away memories that last a lifetime. 🌟✨ Together, let's embrace the spirit of exploration while preserving the wonders of our world for future generations!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--6T695JmYiA",
        "outputId": "afc5c90b-3a6b-4320-a44d-a760c94623f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens = nltk.word_tokenize(text)\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok-3kxSqna_H",
        "outputId": "d9525740-4713-4f26-f923-8c136270838b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['As', 'an', 'avid', 'traveler', '🌍✈️', ',', 'I', \"'m\", 'constantly', 'seeking', 'new', 'adventures', 'and', 'experiences', 'that', 'broaden', 'my', 'horizons', '.', 'Whether', 'it', \"'s\", 'trekking', 'through', 'rugged', 'mountains', '⛰️', ',', 'exploring', 'bustling', 'city', 'streets', '🏙️', ',', 'or', 'relaxing', 'on', 'pristine', 'beaches', '🏖️', ',', 'each', 'journey', 'leaves', 'me', 'with', 'unforgettable', 'memories', 'and', 'valuable', 'lessons', '.', 'However', ',', 'amidst', 'the', 'excitement', ',', 'it', \"'s\", 'essential', 'to', 'prioritize', 'sustainability', 'and', 'responsible', 'tourism', 'practices', '.', '🌱🌿', 'Let', \"'s\", 'tread', 'lightly', 'on', 'this', 'beautiful', 'planet', '🌏', ',', 'leaving', 'only', 'footprints', 'and', 'taking', 'away', 'memories', 'that', 'last', 'a', 'lifetime', '.', '🌟✨', 'Together', ',', 'let', \"'s\", 'embrace', 'the', 'spirit', 'of', 'exploration', 'while', 'preserving', 'the', 'wonders', 'of', 'our', 'world', 'for', 'future', 'generations', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Sentence Tokenization**\n",
        "\n",
        "The NLTK sent_tokenize method splits the paragraph into individual sentences based on punctuation marks. In this case, there's only one sentence, so it's not very informative for this specific paragraph."
      ],
      "metadata": {
        "id": "SDmB9B4ynlF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens = nltk.sent_tokenize(text)\n",
        "print(sent_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S98Z5dninrMG",
        "outputId": "025f6836-bec6-4e33-d2a3-f8e3b489ad6b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"As an avid traveler 🌍✈️, I'm constantly seeking new adventures and experiences that broaden my horizons.\", \"Whether it's trekking through rugged mountains ⛰️, exploring bustling city streets 🏙️, or relaxing on pristine beaches 🏖️, each journey leaves me with unforgettable memories and valuable lessons.\", \"However, amidst the excitement, it's essential to prioritize sustainability and responsible tourism practices.\", \"🌱🌿 Let's tread lightly on this beautiful planet 🌏, leaving only footprints and taking away memories that last a lifetime.\", \"🌟✨ Together, let's embrace the spirit of exploration while preserving the wonders of our world for future generations!\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Punctuation-based Tokenizer**\n",
        "\n",
        "The NLTK WordPunctTokenizer splits the text based on punctuation marks, treating them as separate tokens. This tokenizer is useful for extracting words and punctuation separately."
      ],
      "metadata": {
        "id": "HMAvl_YSnvaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "punct_tokens = tokenizer.tokenize(text)\n",
        "print(punct_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyfCHmTgny3L",
        "outputId": "44ad9f4a-dac7-43d7-a8f4-d81557bcb241"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['As', 'an', 'avid', 'traveler', '🌍✈️,', 'I', \"'\", 'm', 'constantly', 'seeking', 'new', 'adventures', 'and', 'experiences', 'that', 'broaden', 'my', 'horizons', '.', 'Whether', 'it', \"'\", 's', 'trekking', 'through', 'rugged', 'mountains', '⛰️,', 'exploring', 'bustling', 'city', 'streets', '🏙️,', 'or', 'relaxing', 'on', 'pristine', 'beaches', '🏖️,', 'each', 'journey', 'leaves', 'me', 'with', 'unforgettable', 'memories', 'and', 'valuable', 'lessons', '.', 'However', ',', 'amidst', 'the', 'excitement', ',', 'it', \"'\", 's', 'essential', 'to', 'prioritize', 'sustainability', 'and', 'responsible', 'tourism', 'practices', '.', '🌱🌿', 'Let', \"'\", 's', 'tread', 'lightly', 'on', 'this', 'beautiful', 'planet', '🌏,', 'leaving', 'only', 'footprints', 'and', 'taking', 'away', 'memories', 'that', 'last', 'a', 'lifetime', '.', '🌟✨', 'Together', ',', 'let', \"'\", 's', 'embrace', 'the', 'spirit', 'of', 'exploration', 'while', 'preserving', 'the', 'wonders', 'of', 'our', 'world', 'for', 'future', 'generations', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Treebank Word tokenizer**\n",
        "\n",
        "The NLTK TreebankWordTokenizer tokenizes the text following the conventions of the Penn Treebank, which handles contractions and punctuation differently. It splits contractions like \"I'm\" into separate tokens \"I\" and \"'m\" and also treats emojis as separate tokens."
      ],
      "metadata": {
        "id": "ukLagZr5n6Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = tokenizer.tokenize(text)\n",
        "print(treebank_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuGuB074n20-",
        "outputId": "73e7e725-f521-488f-b1c1-446b95c2b467"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['As', 'an', 'avid', 'traveler', '🌍✈️', ',', 'I', \"'m\", 'constantly', 'seeking', 'new', 'adventures', 'and', 'experiences', 'that', 'broaden', 'my', 'horizons.', 'Whether', 'it', \"'s\", 'trekking', 'through', 'rugged', 'mountains', '⛰️', ',', 'exploring', 'bustling', 'city', 'streets', '🏙️', ',', 'or', 'relaxing', 'on', 'pristine', 'beaches', '🏖️', ',', 'each', 'journey', 'leaves', 'me', 'with', 'unforgettable', 'memories', 'and', 'valuable', 'lessons.', 'However', ',', 'amidst', 'the', 'excitement', ',', 'it', \"'s\", 'essential', 'to', 'prioritize', 'sustainability', 'and', 'responsible', 'tourism', 'practices.', '🌱🌿', 'Let', \"'s\", 'tread', 'lightly', 'on', 'this', 'beautiful', 'planet', '🌏', ',', 'leaving', 'only', 'footprints', 'and', 'taking', 'away', 'memories', 'that', 'last', 'a', 'lifetime.', '🌟✨', 'Together', ',', 'let', \"'s\", 'embrace', 'the', 'spirit', 'of', 'exploration', 'while', 'preserving', 'the', 'wonders', 'of', 'our', 'world', 'for', 'future', 'generations', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Tweet Tokenizer**\n",
        "\n",
        "The NLTK TweetTokenizer is designed specifically for tokenizing tweets, which often contain emojis and hashtags. It tokenizes emojis separately from words and punctuation, treating them as individual tokens."
      ],
      "metadata": {
        "id": "d3oa6sAjoBiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tokenizer.tokenize(text)\n",
        "print(tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "712VtGAcoDUx",
        "outputId": "0c13a05e-dff2-4311-e263-4939dc7f17a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['As', 'an', 'avid', 'traveler', '🌍', '✈', '️', ',', \"I'm\", 'constantly', 'seeking', 'new', 'adventures', 'and', 'experiences', 'that', 'broaden', 'my', 'horizons', '.', 'Whether', \"it's\", 'trekking', 'through', 'rugged', 'mountains', '⛰', '️', ',', 'exploring', 'bustling', 'city', 'streets', '🏙', '️', ',', 'or', 'relaxing', 'on', 'pristine', 'beaches', '🏖', '️', ',', 'each', 'journey', 'leaves', 'me', 'with', 'unforgettable', 'memories', 'and', 'valuable', 'lessons', '.', 'However', ',', 'amidst', 'the', 'excitement', ',', \"it's\", 'essential', 'to', 'prioritize', 'sustainability', 'and', 'responsible', 'tourism', 'practices', '.', '🌱', '🌿', \"Let's\", 'tread', 'lightly', 'on', 'this', 'beautiful', 'planet', '🌏', ',', 'leaving', 'only', 'footprints', 'and', 'taking', 'away', 'memories', 'that', 'last', 'a', 'lifetime', '.', '🌟', '✨', 'Together', ',', \"let's\", 'embrace', 'the', 'spirit', 'of', 'exploration', 'while', 'preserving', 'the', 'wonders', 'of', 'our', 'world', 'for', 'future', 'generations', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Multi-Word Expression Tokenizer**\n",
        "\n",
        "The NLTK MWETokenizer is used to tokenize multi-word expressions. Since the traveler paragraph doesn't contain specific multi-word expressions, this tokenizer doesn't provide any additional insights here."
      ],
      "metadata": {
        "id": "6AzjaPQpoHAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "tokenizer = MWETokenizer()\n",
        "mwe_tokens = tokenizer.tokenize(text.split())\n",
        "print(mwe_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBUZ5G-FoLLm",
        "outputId": "fad35767-13bc-4e39-bdff-46ac6e92ce51"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['As', 'an', 'avid', 'traveler', '🌍✈️,', \"I'm\", 'constantly', 'seeking', 'new', 'adventures', 'and', 'experiences', 'that', 'broaden', 'my', 'horizons.', 'Whether', \"it's\", 'trekking', 'through', 'rugged', 'mountains', '⛰️,', 'exploring', 'bustling', 'city', 'streets', '🏙️,', 'or', 'relaxing', 'on', 'pristine', 'beaches', '🏖️,', 'each', 'journey', 'leaves', 'me', 'with', 'unforgettable', 'memories', 'and', 'valuable', 'lessons.', 'However,', 'amidst', 'the', 'excitement,', \"it's\", 'essential', 'to', 'prioritize', 'sustainability', 'and', 'responsible', 'tourism', 'practices.', '🌱🌿', \"Let's\", 'tread', 'lightly', 'on', 'this', 'beautiful', 'planet', '🌏,', 'leaving', 'only', 'footprints', 'and', 'taking', 'away', 'memories', 'that', 'last', 'a', 'lifetime.', '🌟✨', 'Together,', \"let's\", 'embrace', 'the', 'spirit', 'of', 'exploration', 'while', 'preserving', 'the', 'wonders', 'of', 'our', 'world', 'for', 'future', 'generations!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TextBlob Word Tokenizer**\n",
        "\n",
        "TextBlob tokenizes the text into words similar to NLTK's word_tokenize, handling contractions and emojis as separate tokens."
      ],
      "metadata": {
        "id": "XPM0nqhhoNVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "blob = TextBlob(text)\n",
        "textblob_tokens = blob.words\n",
        "print(textblob_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdylmThZoSSe",
        "outputId": "4cafab7b-448e-4a88-befc-a1b186473e5b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['As', 'an', 'avid', 'traveler', '🌍✈️', 'I', \"'m\", 'constantly', 'seeking', 'new', 'adventures', 'and', 'experiences', 'that', 'broaden', 'my', 'horizons', 'Whether', 'it', \"'s\", 'trekking', 'through', 'rugged', 'mountains', '⛰️', 'exploring', 'bustling', 'city', 'streets', '🏙️', 'or', 'relaxing', 'on', 'pristine', 'beaches', '🏖️', 'each', 'journey', 'leaves', 'me', 'with', 'unforgettable', 'memories', 'and', 'valuable', 'lessons', 'However', 'amidst', 'the', 'excitement', 'it', \"'s\", 'essential', 'to', 'prioritize', 'sustainability', 'and', 'responsible', 'tourism', 'practices', '🌱🌿', 'Let', \"'s\", 'tread', 'lightly', 'on', 'this', 'beautiful', 'planet', '🌏', 'leaving', 'only', 'footprints', 'and', 'taking', 'away', 'memories', 'that', 'last', 'a', 'lifetime', '🌟✨', 'Together', 'let', \"'s\", 'embrace', 'the', 'spirit', 'of', 'exploration', 'while', 'preserving', 'the', 'wonders', 'of', 'our', 'world', 'for', 'future', 'generations']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**spaCy Tokenizer**\n",
        "\n",
        "spaCy tokenizes the text into words, handling contractions, punctuation, and emojis as separate tokens."
      ],
      "metadata": {
        "id": "rF5uxMVsoUuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "print(spacy_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhZ0pNUAoYnl",
        "outputId": "1ff78b9b-bda4-49f8-8e90-c38eadd61140"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['As', 'an', 'avid', 'traveler', '🌍', '✈', '️', ',', 'I', \"'m\", 'constantly', 'seeking', 'new', 'adventures', 'and', 'experiences', 'that', 'broaden', 'my', 'horizons', '.', 'Whether', 'it', \"'s\", 'trekking', 'through', 'rugged', 'mountains', '⛰', '️', ',', 'exploring', 'bustling', 'city', 'streets', '🏙', '️', ',', 'or', 'relaxing', 'on', 'pristine', 'beaches', '🏖', '️', ',', 'each', 'journey', 'leaves', 'me', 'with', 'unforgettable', 'memories', 'and', 'valuable', 'lessons', '.', 'However', ',', 'amidst', 'the', 'excitement', ',', 'it', \"'s\", 'essential', 'to', 'prioritize', 'sustainability', 'and', 'responsible', 'tourism', 'practices', '.', '🌱', '🌿', 'Let', \"'s\", 'tread', 'lightly', 'on', 'this', 'beautiful', 'planet', '🌏', ',', 'leaving', 'only', 'footprints', 'and', 'taking', 'away', 'memories', 'that', 'last', 'a', 'lifetime', '.', '🌟', '✨', 'Together', ',', 'let', \"'s\", 'embrace', 'the', 'spirit', 'of', 'exploration', 'while', 'preserving', 'the', 'wonders', 'of', 'our', 'world', 'for', 'future', 'generations', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Gensim word tokenizer**\n",
        "\n",
        "Gensim tokenizes the text into words, treating emojis and punctuation as separate tokens, similar to other word tokenization methods."
      ],
      "metadata": {
        "id": "Yck5bYiCogAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize\n",
        "gensim_tokens = list(tokenize(text))\n",
        "print(gensim_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiEBjIJoofKE",
        "outputId": "c95b4ef6-1e28-4801-9a24-53af16e548a9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['As', 'an', 'avid', 'traveler', 'I', 'm', 'constantly', 'seeking', 'new', 'adventures', 'and', 'experiences', 'that', 'broaden', 'my', 'horizons', 'Whether', 'it', 's', 'trekking', 'through', 'rugged', 'mountains', 'exploring', 'bustling', 'city', 'streets', 'or', 'relaxing', 'on', 'pristine', 'beaches', 'each', 'journey', 'leaves', 'me', 'with', 'unforgettable', 'memories', 'and', 'valuable', 'lessons', 'However', 'amidst', 'the', 'excitement', 'it', 's', 'essential', 'to', 'prioritize', 'sustainability', 'and', 'responsible', 'tourism', 'practices', 'Let', 's', 'tread', 'lightly', 'on', 'this', 'beautiful', 'planet', 'leaving', 'only', 'footprints', 'and', 'taking', 'away', 'memories', 'that', 'last', 'a', 'lifetime', 'Together', 'let', 's', 'embrace', 'the', 'spirit', 'of', 'exploration', 'while', 'preserving', 'the', 'wonders', 'of', 'our', 'world', 'for', 'future', 'generations']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Tokenization with Keras (using Keras Tokenizer)**\n",
        "\n",
        "Keras Tokenizer tokenizes the text into words, treating emojis and punctuation as separate tokens, similar to other word tokenization methods."
      ],
      "metadata": {
        "id": "L-sx1bsWolof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "keras_tokens = tokenizer.texts_to_sequences([text])\n",
        "print(keras_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM9yKICrofS2",
        "outputId": "112b68a3-257d-47f9-9098-02e68e1f7f63"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1, 19, 3, 20, 21, 22, 23, 4, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 5, 36, 37, 38, 39, 40, 41, 42, 43, 44, 6, 1, 45, 46, 47, 48, 2, 49, 4, 50, 51, 52, 53, 1, 54, 55, 56, 57, 7, 58, 59, 5, 60, 61, 62, 63, 64, 65, 66, 1, 67, 68, 6, 3, 69, 70, 71, 72, 73, 7, 74, 2, 75, 8, 76, 77, 78, 2, 79, 8, 80, 81, 82, 83, 84]]\n"
          ]
        }
      ]
    }
  ]
}